{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import col, count, input_file_name, udf, to_date, hour, minute, second\n",
    "from lib.utils import get_spark_app_config, extract_timestamp, find_files_of_type\n",
    "import sys, os, stat, shutil, csv\n",
    "import psycopg2\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument(\"--source_folder\", help=\"Source folder containing initial data\")\n",
    "# parser.add_argument(\"--pg_host\", help=\"PostgreSQL host name\")\n",
    "# parser.add_argument(\"--pg_database\", help=\"PostgreSQL database name\")\n",
    "# parser.add_argument(\"--pg_table\", help=\"PostgreSQL database table\")\n",
    "# parser.add_argument(\"--pg_username\", help=\"PostgreSQL database user name\")\n",
    "# parser.add_argument(\"--pg_password\", help=\"PostgreSQL database user password\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# source_folder = args.source_folder\n",
    "# pg_host = args.pg_host\n",
    "# pg_database = args.pg_database\n",
    "# pg_table = args.pg_table\n",
    "# pg_username = args.pg_username\n",
    "# pg_password = args.pg_password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR StatusLogger Reconfiguration failed: No configuration found for '67424e82' at 'null' in 'null'\n",
      "ERROR StatusLogger Reconfiguration failed: No configuration found for 'Default' at 'null' in 'null'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 12:30:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# TODO 1\n",
    "\n",
    "# Initiate Spark sesion\n",
    "if __name__ == '__main__':\n",
    "    conf = get_spark_app_config()\n",
    "    spark = SparkSession.builder \\\n",
    "        .enableHiveSupport() \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set UDF for extracting timestamp\n",
    "udf_extract_ts = udf(extract_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get args from cli\n",
    "# source_folder = sys.argv[1]\n",
    "# pg_host = sys.argv[2]\n",
    "# pg_username = sys.argv[3]\n",
    "# pg_password = sys.argv[4]\n",
    "source_folder = 'source'\n",
    "pg_host = 'localhost'\n",
    "pg_database = 'click_db'\n",
    "pg_username = 'postgres'\n",
    "pg_password = 'postgres'\n",
    "pg_table='click_rate'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 12:30:10 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# Read impressions Dataframe\n",
    "impressions_df = spark.read \\\n",
    "    .option('inferSchema' , 'true') \\\n",
    "    .parquet(f'{source_folder}/impressions*') \\\n",
    "    .select(col('device_settings')['user_agent']) \\\n",
    "    .withColumnRenamed('device_settings.user_agent', 'user_agent') \\\n",
    "    .withColumn('timestamp', udf_extract_ts(input_file_name())) \\\n",
    "    .withColumn('date', to_date(col('timestamp'))) \\\n",
    "    .withColumn('hour', hour(col('timestamp'))) \\\n",
    "    .withColumn('min', minute(col('timestamp'))) \\\n",
    "    .withColumn('s', second(col('timestamp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read clicks Dataframe\n",
    "clicks_df = spark.read \\\n",
    "    .option('inferSchema' , 'true') \\\n",
    "    .parquet(f'{source_folder}/clicks*') \\\n",
    "    .select(col('device_settings')['user_agent']) \\\n",
    "    .withColumnRenamed('device_settings.user_agent', 'c_user_agent') \\\n",
    "    .withColumn('c_timestamp', udf_extract_ts(input_file_name())) \\\n",
    "    .withColumn('c_date', to_date(col('c_timestamp'))) \\\n",
    "    .withColumn('c_hour', hour(col('c_timestamp'))) \\\n",
    "    .withColumn('c_min', minute(col('c_timestamp'))) \\\n",
    "    .withColumn('c_s', second(col('c_timestamp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join impressions and clicks, count, group by date and hour\n",
    "join_df = impressions_df.join(clicks_df, (impressions_df['date'] == clicks_df['c_date']) & (impressions_df['hour'] == clicks_df['c_hour']), 'left') \\\n",
    "    .groupBy(col('date'), col('hour')) \\\n",
    "    .agg(count('hour').alias('impressions_count'),count('c_hour').alias('click_count')) \\\n",
    "    .orderBy('date', 'hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join_df.createOrReplaceTempView(\"click_rates_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write report data to staging\n",
    "join_df.write \\\n",
    "    .option('header',True) \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv('./staging/')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2\n",
    "\n",
    "# Connect to Docker PostgreSQL Database\n",
    "# Connect to database\n",
    "conn = psycopg2.connect(\n",
    "    database = pg_database,\n",
    "    user= pg_username,\n",
    "    password = pg_password,\n",
    "    host = pg_host\n",
    ")\n",
    "\n",
    "# Open cursor to perform database operation\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create table, or drop if exist for regular report updates\n",
    "create_db_qry = (f'''\n",
    "    DROP TABLE IF EXISTS click_rate;\n",
    "    CREATE TABLE {pg_table}\n",
    "    (\n",
    "    datetime TIMESTAMP,\n",
    "    impression_count BIGINT,\n",
    "    click_count BIGINT,\n",
    "    audit_loaded_datetime TIMESTAMP DEFAULT NOW()   \n",
    "    )\n",
    "    '''   \n",
    ")\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find csv file in staging directory\n",
    "csv_file = find_files_of_type('./staging', '.csv')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write custom report to Postgresql DB\n",
    "with open(f'./staging/{csv_file}', 'r') as f:\n",
    "\n",
    "    # Read csv file, skip header\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    next(reader, None)\n",
    "\n",
    "    # Create new list with specific columns\n",
    "    load_list = [[x for i,x in enumerate(line) if i!=1] for line in reader]\n",
    "  \n",
    "# Insert values to click_rate table       \n",
    "for item in load_list:\n",
    "    cur.execute(f'INSERT INTO {pg_table} (datetime, impression_count, click_count) VALUES (%s, %s, %s)', item)\n",
    "    \n",
    "# Close communications with database\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION USED JDBC CONNECTION\n",
    "\n",
    "# join_df.write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", f\"jdbc:postgresql://{pg_host}:5432/{pg_database}\") \\\n",
    "#     .option(\"dbtable\", \"\\'click_rate'\") \\\n",
    "#     .option(\"user\", \"postgres\") \\\n",
    "#     .option(\"password\", \"postgres\") \\\n",
    "#     .option('driver', 'com.postgresql.Driver') \\\n",
    "#     .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete source files\n",
    "for filename in os.listdir(source_folder):\n",
    "    file_path = os.path.join(source_folder, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print(f'Failed to delete {file_path}. Reason: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (default, Jan 18 2023, 13:02:56) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eeea088d537ce431989c3e95229605305dc03596717134765d71e3fa7c5b94ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
